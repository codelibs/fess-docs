====================
クローラー基本設定
====================

概要
====

|Fess| のクローラーは、Webサイトやファイルシステムなどから自動的にコンテンツを収集し、検索インデックスに登録する機能です。
本ガイドでは、クローラーの基本的な概念と設定方法について説明します。

クローラーの基本概念
====================

クローラーとは
--------------

クローラー(Crawler)は、指定されたURLやファイルパスを起点として、リンクをたどりながら自動的にコンテンツを収集するプログラムです。

|Fess| のクローラーは以下の特徴があります:

- **マルチプロトコル対応**: HTTP/HTTPS、ファイルシステム、SMB、FTPなど
- **スケジュール実行**: 定期的な自動クロール
- **増分クロール**: 変更されたコンテンツのみを更新
- **並列処理**: 複数URLの同時クロール
- **ロボット規約対応**: robots.txt の遵守

クローラーの種類
----------------

|Fess| では、対象に応じて以下のクローラー種別があります。

.. list-table:: クローラー種別
   :header-rows: 1
   :widths: 20 40 40

   * - 種別
     - 対象
     - 用途
   * - **Webクローラー**
     - Webサイト(HTTP/HTTPS)
     - 公開Webサイト、イントラネット内のWebサイト
   * - **ファイルクローラー**
     - ファイルシステム、SMB、FTP
     - ファイルサーバー、共有フォルダー
   * - **データストアクローラー**
     - データベース
     - RDB、CSV、JSONなどのデータソース

クロール設定の作成
==================

基本的なクロール設定の追加
--------------------------

1. **管理画面にアクセス**

   ブラウザで ``http://localhost:8080/admin`` にアクセスし、管理者でログインします。

2. **クローラー設定画面を開く**

   左メニューから「クローラー」→「Web」または「ファイルシステム」を選択します。

3. **新しい設定を作成**

   「新規作成」ボタンをクリックします。

4. **基本情報を入力**

   - **名前**: クロール設定の識別名(例: 社内Wiki)
   - **URL**: クロール開始URL(例: ``https://wiki.example.com/``)
   - **クロール間隔**: クロール実行頻度(例: 1時間ごと)
   - **スレッド数**: 並列クロール数(例: 5)
   - **深さ**: リンクをたどる階層の深さ(例: 3)

5. **保存**

   「作成」ボタンをクリックして設定を保存します。

Webクローラーの設定例
---------------------

社内イントラネットサイトのクロール
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

::

    名前: 社内ポータル
    URL: http://intranet.example.com/
    クロール間隔: 1日1回
    スレッド数: 10
    深さ: 無制限(-1)
    最大アクセス数: 10000

公開Webサイトのクロール
~~~~~~~~~~~~~~~~~~~~~~~

::

    名前: 製品サイト
    URL: https://www.example.com/products/
    クロール間隔: 1週間に1回
    スレッド数: 5
    深さ: 5
    最大アクセス数: 1000

ファイルクローラーの設定例
--------------------------

ローカルファイルシステム
~~~~~~~~~~~~~~~~~~~~~~~~

::

    名前: ドキュメントフォルダー
    URL: file:///home/share/documents/
    クロール間隔: 1日1回
    スレッド数: 3
    深さ: 無制限(-1)

SMB/CIFS(Windowsファイル共有)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

::

    名前: ファイルサーバー
    URL: smb://fileserver.example.com/share/
    クロール間隔: 1日1回
    スレッド数: 5
    深さ: 無制限(-1)

認証情報の設定
--------------

認証が必要なサイトやファイルサーバーにアクセスする場合は、認証情報を設定します。

1. 管理画面で「クローラー」→「認証」を選択
2. 「新規作成」をクリック
3. 認証情報を入力:

   ::

       ホスト名: wiki.example.com
       ポート: 443
       認証方式: Basic認証
       ユーザー名: crawler_user
       パスワード: ********

4. 「作成」をクリック

クロールの実行
==============

手動実行
--------

設定したクロールをすぐに実行する場合:

1. クロール設定一覧で対象の設定を選択
2. 「開始」ボタンをクリック
3. 「スケジューラー」メニューでジョブの実行状況を確認

スケジュール実行
----------------

定期的にクロールを実行する場合:

1. 「スケジューラー」メニューを開く
2. 「Default Crawler」ジョブを選択
3. スケジュール式を設定(Cron形式)

   ::

       # 毎日午前2時に実行
       0 0 2 * * ?

       # 毎時0分に実行
       0 0 * * * ?

       # 月曜から金曜の午後6時に実行
       0 0 18 ? * MON-FRI

4. 「更新」をクリック

クロール状況の確認
------------------

実行中のクロール状況を確認:

1. 「スケジューラー」メニューを開く
2. 実行中のジョブを確認
3. ログで詳細を確認:

   ::

       tail -f /var/log/fess/fess_crawler.log

基本的な設定項目
================

クロール対象の制限
------------------

URLパターンによる制限
~~~~~~~~~~~~~~~~~~~~~

特定のURLパターンのみをクロール対象とする、または除外することができます。

**含めるURLパターン(正規表現):**

::

    # /docs/ 配下のみをクロール
    https://example\.com/docs/.*

**除外するURLパターン(正規表現):**

::

    # 特定のディレクトリを除外
    .*/admin/.*
    .*/private/.*

    # 特定のファイル拡張子を除外
    .*\.(jpg|png|gif|css|js)$

深さの制限
~~~~~~~~~~

リンクをたどる階層の深さを制限:

- **0**: 開始URLのみ
- **1**: 開始URLと、そこからリンクされたページ
- **-1**: 無制限(全てのリンクをたどる)

最大アクセス数
~~~~~~~~~~~~~~

クロールするページ数の上限:

::

    最大アクセス数: 1000

1000ページまでクロールしたら停止します。

並列クロール数(スレッド数)
--------------------------

同時にクロールするURLの数を指定します。

.. list-table:: 推奨スレッド数
   :header-rows: 1
   :widths: 40 30 30

   * - 環境
     - 推奨値
     - 説明
   * - 小規模サイト(〜1万ページ)
     - 3〜5
     - 対象サーバーへの負荷を抑える
   * - 中規模サイト(1万〜10万ページ)
     - 5〜10
     - バランスの良い設定
   * - 大規模サイト(10万ページ以上)
     - 10〜20
     - 高速クロールが必要
   * - ファイルサーバー
     - 3〜5
     - ファイルI/O負荷を考慮

.. warning::
   スレッド数を増やしすぎると、クロール対象のサーバーに過度な負荷をかけます。
   適切な値を設定してください。

クロール間隔
------------

クロールを実行する頻度を指定します。

::

    # 時間指定
    クロール間隔: 3600000  # ミリ秒(1時間)

    # または、スケジューラーで設定
    0 0 2 * * ?  # 毎日午前2時

ファイルサイズの設定
====================

クロールするファイルサイズの上限を設定できます。

取得するファイルサイズの上限
----------------------------

クローラー設定の「設定パラメーター」に以下を追加:

::

    client.maxContentLength=10485760

10MBまでのファイルを取得します。デフォルトでは制限なしです。

.. note::
   大きなファイルをクロールする場合は、メモリ設定も調整してください。
   詳細は :doc:`setup-memory` を参照してください。

インデックスするファイルサイズの上限
------------------------------------

ファイルの種類ごとにインデックスするサイズの上限を設定できます。

**デフォルト値:**

- HTMLファイル: 2.5MB
- その他のファイル: 10MB

**設定ファイル:** ``app/WEB-INF/classes/crawler/contentlength.xml``

::

    <?xml version="1.0" encoding="UTF-8"?>
    <!DOCTYPE components PUBLIC "-//DBFLUTE//DTD LastaDi 1.0//EN"
            "http://dbflute.org/meta/lastadi10.dtd">
    <components namespace="fessCrawler">
            <include path="crawler/container.xml" />

            <component name="contentLengthHelper"
                    class="org.codelibs.fess.crawler.helper.ContentLengthHelper" instance="singleton">
                    <property name="defaultMaxLength">10485760</property><!-- 10M -->
                    <postConstruct name="addMaxLength">
                            <arg>"text/html"</arg>
                            <arg>2621440</arg><!-- 2.5M -->
                    </postConstruct>
                    <postConstruct name="addMaxLength">
                            <arg>"application/pdf"</arg>
                            <arg>5242880</arg><!-- 5M -->
                    </postConstruct>
            </component>
    </components>

PDFファイルを5MBまで処理する設定を追加しています。

.. warning::
   扱うファイルサイズを増やす場合は、クローラーのメモリ設定も増やしてください。

.. note::
   ドキュメントサイズが50MBを超える場合は、OpenSearch側の設定も変更する必要があります。
   OpenSearchではJSONコンテンツの文字列フィールドの最大長がデフォルトで50MBに制限されています。

   ``opensearch.yml`` に以下を追加してください:

   ::

       opensearch.xcontent.string.length.max: 104857600

   上記は100MBに設定する例です。詳細は `OpenSearchのドキュメント <https://docs.opensearch.org/latest/install-and-configure/install-opensearch/index/#important-system-properties>`_ を参照してください。

単語の長さ制限
==============

概要
----

英数字だけの長い文字列や連続する記号は、インデックスサイズの増加やパフォーマンス劣化を引き起こします。
そのため、|Fess| ではデフォルトで以下の制限を設けています:

- **連続する英数字**: 20文字まで
- **連続する記号**: 10文字まで

設定方法
--------

``fess_config.properties`` を編集します。

**デフォルト設定:**

::

    crawler.document.max.alphanum.term.size=20
    crawler.document.max.symbol.term.size=10

**例: 制限を緩和する場合**

::

    crawler.document.max.alphanum.term.size=50
    crawler.document.max.symbol.term.size=20

.. note::
   長い英数字列で検索する必要がある場合(例: シリアル番号、トークンなど)は、
   この値を大きくしてください。ただし、インデックスサイズが増加します。

プロキシの設定
==============

概要
----

イントラネット内から外部サイトをクロールする場合、ファイアウォールにブロックされることがあります。
その場合はプロキシサーバーを経由してクロールします。

設定方法
--------

管理画面のクロール設定で、「設定パラメーター」に以下を追加します。

**基本的なプロキシ設定:**

::

    client.proxyHost=proxy.example.com
    client.proxyPort=8080

**認証が必要なプロキシ:**

::

    client.proxyHost=proxy.example.com
    client.proxyPort=8080
    client.proxyUsername=proxyuser
    client.proxyPassword=proxypass

**特定のホストをプロキシから除外:**

::

    client.nonProxyHosts=localhost|127.0.0.1|*.example.com

システム全体のプロキシ設定
--------------------------

全てのクロール設定で同じプロキシを使用する場合は、環境変数で設定できます。

::

    export http_proxy=http://proxy.example.com:8080
    export https_proxy=http://proxy.example.com:8080
    export no_proxy=localhost,127.0.0.1,.example.com

robots.txt の設定
=================

概要
----

robots.txt は、クローラーに対してクロール可否を指示するファイルです。
|Fess| はデフォルトで robots.txt を遵守します。

設定方法
--------

robots.txt を無視する場合は、``fess_config.properties`` を編集します。

::

    crawler.ignore.robots.txt=true

.. warning::
   外部サイトをクロールする場合は、robots.txt を遵守してください。
   無視すると、サーバーに過度な負荷をかけたり、利用規約に違反する可能性があります。

User-Agent の設定
=================

クローラーのUser-Agentを変更できます。

管理画面での設定
----------------

クロール設定の「設定パラメーター」に追加:

::

    client.userAgent=MyCompanyCrawler/1.0

システム全体の設定
------------------

``fess_config.properties`` で設定:

::

    crawler.user.agent=MyCompanyCrawler/1.0

エンコーディング設定
====================

クロールデータのエンコーディング
--------------------------------

``fess_config.properties`` で設定:

::

    crawler.crawling.data.encoding=UTF-8

ファイル名のエンコーディング
----------------------------

ファイルシステムのファイル名のエンコーディング:

::

    crawler.document.file.name.encoding=UTF-8

クロールのトラブルシューティング
================================

クロールが開始されない
----------------------

**確認事項:**

1. スケジューラーが有効か確認

   - 「スケジューラー」メニューで「Default Crawler」ジョブが有効か確認

2. クロール設定が有効か確認

   - クロール設定一覧で対象の設定が有効になっているか確認

3. ログを確認

   ::

       tail -f /var/log/fess/fess.log
       tail -f /var/log/fess/fess_crawler.log

クロールが途中で停止する
------------------------

**考えられる原因:**

1. **メモリ不足**

   - ``fess_crawler.log`` に ``OutOfMemoryError`` がないか確認
   - クローラーのメモリを増やす(詳細は :doc:`setup-memory`)

2. **ネットワークエラー**

   - タイムアウト設定を調整
   - リトライ設定を確認

3. **クロール対象のエラー**

   - 404エラーが多発していないか確認
   - ログでエラー詳細を確認

特定のページがクロールされない
------------------------------

**確認事項:**

1. **URLパターンの確認**

   - 除外URLパターンに該当していないか確認

2. **robots.txt の確認**

   - 対象サイトの ``/robots.txt`` を確認

3. **認証の確認**

   - 認証が必要なページの場合、認証設定を確認

4. **深さ制限**

   - リンクの階層が深さ制限を超えていないか確認

5. **最大アクセス数**

   - 最大アクセス数に達していないか確認

クロールが遅い
--------------

**対策:**

1. **スレッド数を増やす**

   - 並列クロール数を増やす(ただし、対象サーバーの負荷に注意)

2. **不要なURLを除外**

   - 画像やCSSファイルなどを除外URLパターンに追加

3. **タイムアウト設定を調整**

   - 応答が遅いサイトの場合、タイムアウトを短くする

4. **クローラーのメモリを増やす**

   - 詳細は :doc:`setup-memory`

ベストプラクティス
==================

クロール設定の推奨事項
----------------------

1. **適切なスレッド数の設定**

   対象サーバーに過度な負荷をかけないよう、適切なスレッド数を設定します。

2. **URLパターンの最適化**

   不要なファイル(画像、CSS、JavaScriptなど)を除外することで、
   クロール時間を短縮し、インデックスの品質を向上させます。

3. **深さ制限の設定**

   サイト構造に応じて適切な深さを設定します。
   無制限(-1)は、サイト全体をクロールする場合のみ使用します。

4. **最大アクセス数の設定**

   想定外に大量のページをクロールしないよう、上限を設定します。

5. **クロール間隔の調整**

   更新頻度に応じて適切な間隔を設定します。
   - 頻繁に更新されるサイト: 1時間〜数時間ごと
   - あまり更新されないサイト: 1日〜1週間ごと

スケジュール設定の推奨事項
--------------------------

1. **夜間実行**

   サーバー負荷が低い時間帯(例: 深夜2時)に実行します。

2. **重複実行の回避**

   前回のクロールが完了してから次のクロールを開始するよう設定します。

3. **エラー時の通知**

   クロール失敗時にメール通知を設定します。

参考情報
========

- :doc:`crawler-advanced` - クローラー詳細設定
- :doc:`crawler-thumbnail` - サムネイル設定
- :doc:`setup-memory` - メモリ設定
- :doc:`admin-logging` - ログ設定
